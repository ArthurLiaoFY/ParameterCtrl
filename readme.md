# CSTR 模擬環境與 DDPG 強化學習

本專案實現了一個基於 **連續攪拌槽反應器（CSTR）** 的模擬環境，並使用 **Deep Deterministic Policy Gradient (DDPG)** 演算法進行訓練，探索最佳的控制策略。

---

## 功能簡介

### 1. **CSTR 動態模型 (`cstr_system`)**
`cstr_system` 定義了 CSTR 系統的動態行為，使用**常微分方程（ODEs）**描述反應器內部狀態的變化。

#### 狀態變數
- $C_A$: 反應物 A 的濃度   
- $C_B$: 反應物 B 的濃度 
- $T_R$: 反應器內的溫度  
- $T_K$: 冷卻夾套的溫度  

#### 操控輸入
- $F$: 流量 
- $Q_{\text{dot}}$: 加熱功率  

#### 參數描述
- **反應動力學參數**: 反應速率常數、活化能、反應焓變等。  
- **物理參數**: 熱容量、密度、熱傳導係數等，用於描述反應器內的熱量交換與物質轉換。  

---

### 2. **CSTREnv 環境類別**
`CSTREnv` 是基於 CSTR 模型構建的強化學習環境，提供了一個可以進行動態控制和學習的接口。

#### 主要功能
1. **環境初始化 (`__init__`)**
   - 設定初始條件（如濃度、溫度、流量）。
   - 可自定義隨機種子以便重現結果。

2. **狀態重置 (`reset`)**
   - 重置環境到初始狀態，包括濃度、溫度與控制變數。

3. **標準化狀態與動作**
   - **`normed_state`**: 提供標準化的狀態表示，用於提升模型學習效率。
   - **`norm_action`** 和 **`revert_normed_action`**: 用於標準化與反標準化控制輸入。

4. **步進函數 (`step`)**
   - 根據輸入動作（流量與加熱功率）更新系統狀態。
   - 計算獎勵（Reward）：根據系統狀態與理想目標的偏差來決定。

#### 獎勵函數
- 將系統狀態（濃度、溫度）與目標值的偏差最小化，以指導強化學習代理學習最優控制策略。

---

## 使用案例

# CSTR 模擬環境與 DDPG 強化學習

本專案實現了一個基於 **連續攪拌槽反應器（CSTR）** 的模擬環境，並使用 **Deep Deterministic Policy Gradient (DDPG)** 演算法進行訓練，探索最佳的控制策略。

---

## **DDPG 強化學習演算法**
採用 Deep Deterministic Policy Gradient (DDPG) 演算法，專門設計用於處理連續型狀態和動作空間。

#### 演算法特性
- **Actor-Critic 結構**:  
  - Actor 負責決定動作，通過策略網路（Policy Network）生成適合當前狀態的連續型動作。
  - Critic 負責評估 Actor 的動作選擇，通過值網路（Value Network）計算狀態-動作值（Q-value），指導 Actor 改善策略。
- **Replay Buffer（經驗回放緩衝區）**:  
  -	用於存儲代理與環境交互的經驗數據（狀態、動作、獎勵、下一狀態）。
  -	在每次訓練時，從緩衝區中隨機抽取一批樣本進行批量學習，避免樣本間的時間相關性，從而提高學習的穩定性與效率。
  -	支援持久化存儲，使模型可在多次訓練間保留經驗數據。

### 3. **訓練流程**
提供了模組化的訓練流程，包括訓練和推論功能：

#### 訓練 (`train_agent`)
- 重複進行多輪訓練，利用經驗回放緩衝區（Replay Buffer）進行批量更新。  
- 定期執行推論，記錄系統狀態和控制輸入軌跡。

#### 推論 (`inference_once`)
- 使用訓練好的策略對環境進行推論，評估策略性能並記錄結果。
